{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries....\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of images....\n",
    "images_list = os.listdir('dataset/flickr30k_images/flickr30k_images/')\n",
    "print(len(images_list))\n",
    "\n",
    "image_size = (224, 224)\n",
    "num_channels = 3\n",
    "\n",
    "# Taking only the first 5000 images.....\n",
    "sample_size = 5000\n",
    "sample_images_list = images_list[:sample_size]\n",
    "\n",
    "# Reading the images from the images source path....\n",
    "images = []\n",
    "for img_name in sample_images_list:\n",
    "    images.append(plt.imread('dataset/flickr30k_images/flickr30k_images/' + img_name))\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "# Reshaping the images for compatibilty with VGG16's input.....\n",
    "for i in range(images.shape[0]):\n",
    "    images[i] = cv2.resize(images[i], image_size)\n",
    "    images[i] = images[i].reshape(1, image_size[0], image_size[1], num_channels)\n",
    "\n",
    "# Vertically stacking the images....\n",
    "images = np.vstack(images[:])\n",
    "print(images.shape)  # shape is (5000,224,224,3)\n",
    "\n",
    "plt.imshow(images[12])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the image captions from results.csv file.....\n",
    "images_caption = pd.read_csv('dataset/flickr30k_images/results.csv', delimiter='|')\n",
    "images_caption.columns = ['image_name', 'comment_number', 'comment']\n",
    "images_caption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the captions as a list....\n",
    "def get_captions(images_list, images_caption):\n",
    "    captions_list = []\n",
    "    \n",
    "    for img_name in images_list:\n",
    "        captions_list.append(images_caption[images_caption['image_name'] == img_name]['comment'].iat[0])\n",
    "        \n",
    "    return captions_list\n",
    "\n",
    "captions = np.array(get_captions(sample_images_list, images_caption))\n",
    "print(\"Total Captions :\", len(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the captions as a list of lists(all captions for an image in one list).....\n",
    "temp = images_caption.groupby('image_name')['comment'].apply(list).reset_index(name='comment')\n",
    "df = pd.DataFrame(temp, columns= ['comment'])\n",
    "captions_listoflist = df.values.tolist()\n",
    "captions_listoflist = captions_listoflist[:5000]\n",
    "captions_listoflist = np.array(captions_listoflist).reshape(5000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained VGG16 model as an encoder......\n",
    "image_model = VGG16(include_top=True, weights='imagenet') \n",
    "#image_model = InceptionV3(weights='imagenet', include_top=True)\n",
    "image_model.summary()\n",
    "\n",
    "# Input and output for the new model.....\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.get_layer('fc2') \n",
    "#hidden_layer = image_model.get_layer('avg_pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer values size for input to the initial states of decoder model....\n",
    "transfer_values_size = K.int_shape(hidden_layer.output)[1]\n",
    "print(transfer_values_size)\n",
    "\n",
    "# Modified encoder model for getting the tranfer values of the images.....\n",
    "image_features_extract_model = tf.keras.Model(inputs=new_input,outputs=hidden_layer.output)\n",
    "image_features_extract_model.summary()\n",
    "transfer_values = image_features_extract_model.predict(images,batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a start and end word to all the captions.....\n",
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'\n",
    "\n",
    "def mark_captions(captions_listoflist):\n",
    "    captions_marked =  [[mark_start + caption + mark_end\n",
    "                        for caption in caption_list] for caption_list in captions_listoflist]\n",
    "    return captions_marked\n",
    "\n",
    "captions_train = mark_captions(captions_listoflist)\n",
    "print(captions_train[0])\n",
    "\n",
    "\n",
    "# Flattening the list of lists....\n",
    "captions_train_flatten = [caption for captions_list in captions_train for caption in captions_list]\n",
    "print(captions_train_flatten[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer class with various methods and Properties.....\n",
    "# vocab size....\n",
    "num_words = 10000\n",
    "class Tokenizer_Prop(Tokenizer):\n",
    "    def __init__(self,text,num_words=None):\n",
    "        Tokenizer.__init__(self,num_words=num_words)\n",
    "        self.fit_on_texts(text)\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),self.word_index.keys()))\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        text = \" \".join(words)\n",
    "        return text\n",
    "\n",
    "    def captions_to_tokens(self, captions_listlist):\n",
    "        tokens = [self.texts_to_sequences(captions_list) for captions_list in captions_listoflist]\n",
    "        return tokens\n",
    "\n",
    "# Creating an object of the Tokenizer_Prop class....\n",
    "tokenizer = Tokenizer_Prop(text=captions_train_flatten,num_words=num_words)\n",
    "\n",
    "# token for the start word....\n",
    "token_start = tokenizer.word_index[mark_start.strip()]\n",
    "print(token_start)\n",
    "\n",
    "# Token for the end word....\n",
    "token_end = tokenizer.word_index[mark_end.strip()]\n",
    "print(token_end)\n",
    "\n",
    "# Converting the captions to tokens.....\n",
    "token_train = tokenizer.captions_to_tokens(captions_train)\n",
    "print(token_train[0])\n",
    "\n",
    "def get_random_caption_tokens(idx):\n",
    "    result = []\n",
    "    for i in idx:\n",
    "        j = np.random.choice(len(token_train[i]))\n",
    "        tokens = token_train[i][j]\n",
    "        result.append(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size):\n",
    "    while True:\n",
    "        # Get a list of random indices for images in the training-set....\n",
    "        idx = np.random.randint(len(captions_train), size=batch_size)\n",
    "        transfer_values_temp = transfer_values[idx]\n",
    "\n",
    "        # Select one of the 5 captions for the selected image at random and get the associated sequence of integer-tokens...\n",
    "        tokens = get_random_caption_tokens(idx)\n",
    "\n",
    "        # Count the number of tokens in all these token-sequences.\n",
    "        num_tokens = [len(t) for t in tokens]\n",
    "\n",
    "        # Max number of tokens.\n",
    "        max_tokens = np.max(num_tokens)\n",
    "\n",
    "        tokens_padded = pad_sequences(tokens,maxlen=max_tokens, padding='post',truncating='post')\n",
    "\n",
    "        # The decoder-part of the neural network will try to map the token-sequences to themselves shifted one time-step....\n",
    "        decoder_input_data = tokens_padded[:, 0:-1]\n",
    "        decoder_output_data = tokens_padded[:, 1:]\n",
    "\n",
    "        # Dict for the input-data. Because we have several inputs, we use a named dict to ensure that the data is assigned correctly....\n",
    "        x_data = \\\n",
    "        {\n",
    "            'decoder_input': decoder_input_data,\n",
    "            'transfer_values_input': transfer_values_temp\n",
    "        }\n",
    "\n",
    "        # Dict for the output-data.\n",
    "        y_data = \\\n",
    "        {\n",
    "            'decoder_output': decoder_output_data\n",
    "        }\n",
    "\n",
    "        yield (x_data, y_data)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "generator = batch_generator(batch_size=batch_size)\n",
    "batch = next(generator)\n",
    "batch_x = batch[0]\n",
    "batch_y = batch[1]\n",
    "print(batch_x['transfer_values_input'][0])\n",
    "\n",
    "num_captions_train = [len(captions) for captions in captions_listoflist]\n",
    "total_num_captions_train = np.sum(num_captions_train)\n",
    "steps_per_epoch = int(total_num_captions_train / batch_size)\n",
    "print(steps_per_epoch)\n",
    "\n",
    "state_size = 512\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "transfer_values_input = Input(shape=(transfer_values_size,),name='transfer_values_input')\n",
    "\n",
    "decoder_transfer_map = Dense(state_size,activation='tanh',name='decoder_transfer_map')\n",
    "\n",
    "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
    "\n",
    "decoder_embedding = Embedding(input_dim=num_words,output_dim=embedding_size,name='decoder_embedding')\n",
    "\n",
    "\n",
    "decoder_gru1 = GRU(state_size, name='decoder_gru1',return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2',return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3',return_sequences=True)\n",
    "\n",
    "decoder_dense = Dense(num_words,activation='softmax',name='decoder_output')\n",
    "\n",
    "def connect_decoder(transfer_values):\n",
    "    # Map the transfer-values so the dimensionality matches the internal state of the GRU layers....\n",
    "    initial_state = decoder_transfer_map(transfer_values)\n",
    "\n",
    "    # Start the decoder-network with its input-layer.....\n",
    "    net = decoder_input\n",
    "\n",
    "    # Connect the embedding-layer....\n",
    "    net = decoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU layers....\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "\n",
    "    # Connect the final dense layer that converts to one-hot encoded arrays....\n",
    "    decoder_output = decoder_dense(net)\n",
    "\n",
    "    return decoder_output\n",
    "\n",
    "decoder_output = connect_decoder(transfer_values=transfer_values_input)\n",
    "\n",
    "decoder_model = Model(inputs=[transfer_values_input, decoder_input],outputs=[decoder_output])\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.compile(optimizer=RMSprop(lr=1e-3),loss='sparse_categorical_crossentropy')\n",
    "decoder_model.fit(x=generator, steps_per_epoch=steps_per_epoch, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(max_tokens=50):\n",
    "    # Load and resize the image\n",
    "    test_image = plt.imread('dataset/flickr30k_images/flickr30k_images/'+ images_list[2200])\n",
    "    print(test_image.shape)\n",
    "    test_image = cv2.resize(test_image, image_size)\n",
    "    # Expand the 3-dim numpy array to 4-dim as the image-model expects a whole batch as input....\n",
    "    image_batch = np.expand_dims(test_image, axis=0)\n",
    "    print(image_batch.shape)\n",
    "\n",
    "    # Process the image with the pre-trained image-model to get the transfer-values....\n",
    "    transfer_values_test = image_features_extract_model.predict(image_batch)\n",
    "\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
    "    # This holds just a single sequence of integer-tokens,\n",
    "    # but the decoder-model expects a batch of sequences.\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "    \n",
    "    list_of_indices = []\n",
    "\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        # Update the input-sequence to the decoder with the last token that was sampled....\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        # Wrap the input-data in a dict for clarity and safety,\n",
    "        # so we are sure we input the data in the right order.\n",
    "        x_data = \\\n",
    "        {\n",
    "            'transfer_values_input': transfer_values_test,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "\n",
    "        # Input this data to the decoder and get the predicted output....\n",
    "        decoder_output = decoder_model.predict(x_data)\n",
    "\n",
    "        # Get the last predicted token as a one-hot encoded array....\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "\n",
    "        # Convert to an integer-token.....\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        \n",
    "        list_of_indices.append(token_int)\n",
    "\n",
    "        # Increment the token-counter.....\n",
    "        count_tokens += 1\n",
    "\n",
    "    # This is the sequence of tokens output by the decoder....\n",
    "    words = [reverse_word_map.get(word) for word in list_of_indices if word]\n",
    "    output_text = ' '.join(words)\n",
    "\n",
    "    # Plot the image.....\n",
    "    plt.imshow(test_image)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the predicted caption....\n",
    "    print(\"Predicted caption:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "generate_caption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.save('image_dec_vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.save('image_dec_vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('image_reverse_word_map.json', 'w') as f:\n",
    "    json.dump(reverse_word_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
